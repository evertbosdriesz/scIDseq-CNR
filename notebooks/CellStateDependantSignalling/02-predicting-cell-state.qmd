---
title: "Predicting cell state from signaling activity"
author: "Evert Bosdriesz"
toc: true
format:
  html:
    html-math-method: katex
    code-fold: true
    self-contained: true
    df-print: paged
execute:
  warning: false
---

<!-- ## OVERVIEW -->

## Background

Cell state markers are defined as antibodies that show no differential expression upon either treatment.
We've clustered cells based on cell state markers. 
A heatmap of the resulting clustering indicates that the signaling markers, which were not used in determining the clusters, nonetheless showed clear cell state specific expression patterns. 
This raises the question to which extend cell state information is present in the signaling state. 
To study this question, we will train cell state classifiers using the signaling state markers as features. 
We assess the performance of the classifiers using 5-fold cross-validation.

## Loading and exploring the data

```{r init, echo=FALSE, cache=FALSE, message=FALSE}
library(tidyverse)
library(here)
library(glmnet)
library(tidymodels)
library(dotwhisker)
library(ranger)
library(ggplot2)
library(furrr)
library(future.apply)
plan(multisession)

source(here("src", "graphics-options.R"))
 
## Global options
options(max.print = "80")
```


```{r load-data-and-annotations}
dat_zscore <- read_csv(
  here("data", "processed" ,"TMM_normalised_z_transformed_concencus_clusters.csv"),
  show_col_types = FALSE) |> 
  select(-...1, -plate_number) |> # Keep treatment and sample ID
  mutate(
    cluster = as_factor(cluster),
    treatment = as_factor(treatment)) 

cell_state_markers <- read_lines(here("data", "annotations", "cell_state_markers.txt"))
signaling_state_markers <- read_lines(here("data", "annotations", "signalling_state_markers.txt"))
```

## Overview of the data

Cluster 2 has most cells (90) and cluster 9 has only 33 cells. Overall most clusters have between 50 and 60 cells.
```{r data-overview}
sampleSize <- data.frame(matrix(nrow = 0, ncol = 2))
colnames(sampleSize) <- c("Cluster", "Size")
for(i in seq(1:9))
{
  sampleSize[nrow(sampleSize) + 1,] = c(i, nrow(dat_zscore[dat_zscore$cluster == i,]))
}
knitr::kable(sampleSize)
```

<!-- ## Evaluating model performance  -->

## Training the model

We will train two types of models. 
Predicting the cell state from the cell state markers (as a positive control) and predicting it from the signaling state markers **that were not used in defining the cell state*.

```{r dat_sigstate}
dat_cellstate <- select(dat_zscore, all_of(c("cluster", "sample_id", 
                                             "treatment", cell_state_markers)))
dat_sigstate <- select(dat_zscore, all_of(c("cluster", "sample_id", 
                                            "treatment", signaling_state_markers)))
```

We will use

  * Multinomal logistic regression
  * Random forrest
  * Graph Convolutional Neural Network (GCNN) (output generated in python notebook)

We are running a 5-fold (inner & outer) nested cross-validation with 5 repeats. For the random forest we trained the hyperparameters mtry, min_n, trees with 3 parameter options for each in the inner fold. For the MLR model we optimized the penalty and model-type (mixture) in a randomly initialized grid with 50 combinations. 
```{r RF-models, echo=FALSE}

#inspect the model performace when a single parameter changes (to guide, e.g., grid search parameter estimation)
inspect_single_parameter <- function(parameters, cv_folds, variable)
{
  # random forest model: outcome metric accuracy
  rf_mod <- function(parameter, object) 
  {
    #dynamically define model based on parameter input
    if(variable == "trees"){model_def <- rand_forest(trees = parameter)}
    else if(variable == "mtry"){model_def <- rand_forest(mtry = parameter)}
    else if(variable == "min_n"){model_def <- rand_forest(min_n = parameter)}
    else{print("ERROR: INVALID PARAMETER!")}
    
    model <- 
      model_def %>% 
      set_engine("ranger") %>% 
      set_mode("classification")
    fit <- model %>%
       fit(cluster ~ ., data = analysis(object))
    
    holdout_pred <- 
      predict(fit, assessment(object) %>% dplyr::select(-cluster)) %>% 
      bind_cols(assessment(object) %>% dplyr::select(cluster))
    # add accuracy calculation
    accuracy <- yardstick::accuracy(holdout_pred, truth = cluster, .pred_class)$.estimate
  }
  # call rf model for all hyperparameter values
  tune_over_parameters <- function(object) 
  {
    tibble(parameters = parameters) %>% 
      mutate(ACCURACY = map_dbl(parameters, rf_mod, object = object))
  }
  # summarise the output metrics
  summarize_tune_results <- function(object) 
  {
    # Return row-bound tibble that has the 25 bootstrap results
    tune_over_parameters(object) %>%
      # For each value of the tuning parameter, compute the 
      # average RMSE which is the inner bootstrap estimate. 
      group_by(parameters) %>%
      summarize(mean_ACCURACY = mean(ACCURACY, na.rm = TRUE),
                n = length(ACCURACY),
                .groups = "drop")
  }
  #run final nested cross validation
  tuning_results <- future_map(cv_folds$splits, summarize_tune_results) 
  
  #plot accuracy against parameter values for all folds/ repeats
  pooled_inner <- tuning_results |>
    dplyr::bind_rows()
  best_cost <- function(dat) dat[which.min(dat$mean_ACCURACY),]
  p <- 
    ggplot(pooled_inner, aes(x = parameters, y = mean_ACCURACY)) + 
    scale_x_continuous() +
    xlab("Parameter Values") + ylab("mean accuracy")
  for (i in 1:length(tuning_results))
    p <- p  +
      geom_line(data = tuning_results[[i]], alpha = .2) +
      geom_point(data = best_cost(tuning_results[[i]]), pch = 16, alpha = 3/4)
  p <- p + geom_smooth(data = pooled_inner, se = FALSE)
  p
}

#random forest model, tuning min_n, mtry and number of trees
rf_mod_grid_search <- rand_forest(min_n = tune(), mtry = tune(), trees = tune()) %>% 
    set_engine("ranger") %>% 
    set_mode("classification")
#Multinomial Logistic regression
mlr_mod <- 
  multinom_reg(penalty = tune(), mixture = tune()) |> 
  set_engine("glmnet") |>
  set_mode("classification")


predict_performance_of_outerFold <- function(object, model, data, hyperparams) 
{
  #folds
  innerFold <- object$inner_resamples
  outerFold <- object$splits

  #use the innerFolds for parameter optimization via grid search
  rf_rec <- recipe(cluster ~ ., data = data) 
  rf_wf <-  workflow() |> 
    add_model(model) |>
    add_recipe(rf_rec) 

  # fit parameters
  print("RUNNING GRID SEARCH")
  spline_res <-
    tune_grid(rf_wf, resamples = innerFold, grid = hyperparams)
  #inspect results
  print(show_best(spline_res, metric = "accuracy"))
  
  #final workflow from whole inner fold
  best_param <- select_best(spline_res,metric = "accuracy")
  final_wf <- finalize_workflow(rf_wf, best_param)
  #fit final model to whole inner fold
  final_res <- final_wf |> 
    fit(analysis(outerFold))
  #predict for outer fold
  final_pre <-  final_res |>
    predict(new_data = assessment(outerFold)) |> 
    bind_cols(assessment(outerFold)) 
  
  #accuracy
  accuracy <- final_pre |> 
    yardstick::accuracy(truth = cluster, estimate = .pred_class)
  #ROC AUC
  annotated_probabilities <- augment(final_res, assessment(outerFold))
  roc_auc <- annotated_probabilities |> 
    roc_auc(cluster, .pred_1:.pred_9, estimator = "hand_till")
  roc_curve <- annotated_probabilities |> 
    roc_curve(cluster, .pred_1:.pred_9) %>%
    autoplot()
  #get also confusion matrix
  conf_m <- final_pre |> 
    conf_mat(truth = cluster, estimate = .pred_class)

  return(list("accuracy" = accuracy$.estimate, "ROC_AUC" = roc_auc$.estimate, "ROC_CURVE" = roc_curve, "confusion_matrix" = conf_m))
}
predict_performance <- function(data, model, hyperparams)
{
  # cross validation scheme
  nested_cellstate_folds <- nested_cv(data, 
                     outside = vfold_cv(v = 5, repeats = 5, strata = cluster), 
                     inside = vfold_cv(v = 5, repeats = 1, strata = cluster)) 

  #run final nested cross validation
  tuning_results <- future_apply(nested_cellstate_folds, 1, predict_performance_of_outerFold, model=model, data = data, hyperparams = hyperparams, future.seed=TRUE) 
}

visualize_performance <- function(results)
{
  accuracy <- list()
  roc_auc <- list()
  avg_conf_m <- NULL
  count <- 0
  for(i in results)
  {
    count <- count + 1
    accuracy <- append(accuracy, i$accuracy)
    roc_auc <- append(roc_auc, i$ROC_AUC)
    if(is.null(avg_conf_m)){avg_conf_m <- broom::tidy(i$confusion_matrix) %>% select(-value)}
    else
    {
      number <- as.character(count)
      new_conf_m <- broom::tidy(i$confusion_matrix) %>%
        mutate("val_{number}" := value) %>%
        select(-value)
      avg_conf_m <- merge(avg_conf_m, new_conf_m,by="name")
    }
  }
  avg_conf_m <- avg_conf_m %>%
    mutate(sum = rowSums(across(where(is.numeric)), na.rm=TRUE))
  avg_conf_m[c('Cell', 'X', 'Y')] <- str_split_fixed(avg_conf_m$name, '_', 3)
  avg_conf_m <- avg_conf_m %>%
    select(c(sum, X, Y))
  
  print(paste0("The average accuracy is ", as.character(mean(unlist(accuracy)))))
  print(paste0("The average ROC AUC is ", as.character(mean(unlist(roc_auc)))))

  ggplot(data = avg_conf_m, aes(X, forcats::fct_rev(Y), fill= sum)) + 
    geom_tile() + geom_text(aes(label=sum)) + 
      scale_fill_gradient2(low = "white", mid = "white", high = "blue")
}

```

### Random forest

```{r hyperparameter inspection, eval=FALSE}
#Inspecting accuracy for single parameter changes

data_sig <- dat_sigstate %>% select(-c("sample_id", "treatment"))
fold <- vfold_cv(data_sig, repeats = 5, strata = cluster)
#INFLUENCE OF MIN_N
parameters <- c(2, 5, 10, 15, 20)
inspect_single_parameter(parameters, fold, "min_n")
#INFLUENCE OF MTRY
parameters <- c(2, 5, 10, 15, 20)
inspect_single_parameter(parameters, fold, "mtry")
#INFLUENCE OF TREE NUMBER
parameters <- c(50, 100, 500, 1000, 2000, 4000)
inspect_single_parameter(parameters, fold, "trees")
```

As a baseline model we predict the cluster from the cell state markers - which were used to define the clusters; How well can we predict the cell state from the cell state markers? As expected, quite well.

This results in in an average accuracy of 83 and ROC AUC of 98 for the baseline model predicting cell state from cell state markers.
For predicting the cell state from signaling markers we achieve an accuracy of 52 and ROC AUC of 87.
```{r rf-predictions}

#parameters
hyperparams <- expand.grid(min_n = c(2, 5,10), mtry = c(2, 5,10), trees = c(500, 1000, 2000))

#data_cell <- dat_cellstate %>% select(-c("sample_id", "treatment"))
#estimates_cellstate_rf <- predict_performance(data_cell, rf_mod_grid_search, hyperparams)
#save(estimates_cellstate_rf, file = here("data", "cellstate-prediction", "Results_CellState_RF.RData"))
estimates_cellstate_rf <- get(load(here("data", "cellstate-prediction", "Results_CellState_RF.RData")))

#data_sig <- dat_sigstate %>% select(-c("sample_id", "treatment"))
#estimates_sigstate_rf <- predict_performance(data_sig, rf_mod_grid_search, hyperparams)
#save(estimates_sigstate_rf, file = here("data", "cellstate-prediction", "Results_SigState_RF.RData"))
estimates_sigstate_rf <- get(load(here("data", "cellstate-prediction", "Results_SigState_RF.RData")))

# Visualizations of results
visualize_performance(estimates_cellstate_rf)

visualize_performance(estimates_sigstate_rf)

```

Following plots show examplary ROC-CURVES for the 9 differences classes  for the first outer-fold of the baseline (cell state) and signaling state model:
```{r ROC-CURVES}
print("Cell State Marker Prediction")
estimates_cellstate_rf[[1]]$ROC_CURVE

print("Signaling State Marker Prediction")
estimates_sigstate_rf[[1]]$ROC_CURVE

```


These results seem consistent with Klaas' analysis.

### Multinomial logistic regression

For Multinomial logistic regression we tune mixture (model type) together with the penalty over a semi-random grid with 50 combinations.

```{r MLR-models, echo=FALSE}
mlr_mod <- 
  multinom_reg(penalty = tune(), mixture = tune()) |> 
  set_engine("glmnet") |>
  set_mode("classification")

```


```{r MLR-predictions}

#MLR
hyperparams <- 50
#data_sig <- dat_sigstate %>% select(-c("sample_id", "treatment"))
#estimates_sigstate_mlr <- predict_performance(data_sig, mlr_mod, hyperparams)
#save(estimates_sigstate_mlr, file = here("data", "cellstate-prediction", "Results_SigState_MLR.RData"))
estimates_sigstate_mlr <- get(load(here("data", "cellstate-prediction", "Results_SigState_MLR.RData")))

visualize_performance(estimates_sigstate_mlr)

```

### GCNN





<!-- ## Conclusion -->
